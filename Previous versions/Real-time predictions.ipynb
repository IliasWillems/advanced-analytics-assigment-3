{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4470ab",
   "metadata": {},
   "source": [
    "# Predicting in real-time\n",
    "\n",
    "In this notebook we will build predictive models and run them in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24d352",
   "metadata": {},
   "source": [
    "## Set-up Spark\n",
    "\n",
    "Copy-paste from example scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fae2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd6d0a",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Some notes:\n",
    "* We will define two models\n",
    "* For each model, we define its own `process` function since the general structure of how the predictions are obtained and appended to the dataframe changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ace9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "import pyspark.sql.types as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bc696",
   "metadata": {},
   "source": [
    "### VADER sentiment analyzer\n",
    "This is a model that we didn't even had to train, as it uses the VADER sentiment scores to score each text on how positive/negative it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdf9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to apply VADER sentiment analysis to a text string\n",
    "def get_sentiment(text):\n",
    "    text_str = str(text) # convert to string\n",
    "    \n",
    "    sentiment = analyzer.polarity_scores(text_str)\n",
    "    if sentiment['compound'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "udf_VADER = udf(get_sentiment, tp.StringType())\n",
    "\n",
    "def process_VADER(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        print(\"rdd was empty...\")\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "\n",
    "    df_withpreds = df.withColumn(\"pred\", udf_VADER( struct(df.review_text) ))\n",
    "    \n",
    "    df_withpreds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192167b",
   "metadata": {},
   "source": [
    "### Logistic regression model\n",
    "\n",
    "General structure (data pipeline):\n",
    "* Tokenize review\n",
    "* 'Translate' words into vector representation\n",
    "* Apply a logistic regression model to with vector representations as regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358c05ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+\n",
      "|review_id| app_id|         review_text|label|\n",
      "+---------+-------+--------------------+-----+\n",
      "|136510302|2103530|I simply love it,...|    1|\n",
      "|136509602|2349550|  Gifted word: Grace|    1|\n",
      "|136510134|1685730|It's pretty good!...|    1|\n",
      "|136510117|1685730|I recommend it, I...|    1|\n",
      "|136509657|2364130|simple point and ...|    0|\n",
      "+---------+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- review_id: integer (nullable = true)\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "# Read in the data set. \n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'review_id', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'app_id', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'review_text', dataType= tp.StringType(), nullable= True),\n",
    "  tp.StructField(name= 'label', dataType= tp.IntegerType(), nullable= True)\n",
    "])\n",
    "my_data = spark.read.csv(\"C:/Users/wille/spark/MyData/review_data.csv\",\n",
    "                         schema=my_schema,\n",
    "                         header=True)\n",
    "\n",
    "# Probably because of the way the data set was stored as a .csv-file, the extremely long reviews are stored improperly,\n",
    "# leading to missing values. Therefore, we drop these cases from the data set.\n",
    "my_data = my_data.dropna()\n",
    "\n",
    "# view the data\n",
    "my_data.show(5)\n",
    "\n",
    "# print the schema of the file\n",
    "print(\"Schema:\")\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17852dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model will consist of several stages, elaborated further below.\n",
    "\n",
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'review_text' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 50\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize=50)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9aeace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we combine each of these stages together into a data pipeline...\n",
    "pipeline = Pipeline(stages = [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# and fit the model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee395422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, transform\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def process_logistic_regression(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        print(\"rdd was empty\")\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Make prediction on data frame based on learned model\n",
    "    out = pipelineFit.transform(df).select(\"prediction\")\n",
    "    \n",
    "    # Combine the prediction (which is a data frame) with the original data frame. After searching for a long time on\n",
    "    # how to do this, the only solution we found is this incredibly convoluted way (first append index column to both\n",
    "    # data frames, then merge them based on this index column, then drop the index column).\n",
    "    out = out.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    df = df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    df_withpreds = df.join(out, on=[\"row_index\"]).drop(\"row_index\")\n",
    "    \n",
    "    # Show data frame again, alongside the predictions.\n",
    "    df_withpreds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038a59e",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "Now we are ready to make real-time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b737716",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "\n",
    "# Fill in either 'process_VADER' or 'process_logistic_regression'\n",
    "lines.foreachRDD(process_logistic_regression)\n",
    "\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69cfe112",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76716ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
