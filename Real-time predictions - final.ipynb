{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4470ab",
   "metadata": {},
   "source": [
    "# Predicting in real-time\n",
    "\n",
    "In this notebook we will build predictive models and run them in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24d352",
   "metadata": {},
   "source": [
    "## 1. Set-up Spark\n",
    "\n",
    "Copy-paste from example scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fae2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd6d0a",
   "metadata": {},
   "source": [
    "## 2. Define the models\n",
    "\n",
    "Some notes:\n",
    "* We will define two models: VADER sentiment analyzer and logistic regression model\n",
    "* For each model, we define its own `process` function since the general structure of how the predictions are obtained and appended to the dataframe changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ace9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "import pyspark.sql.types as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bc696",
   "metadata": {},
   "source": [
    "### 2.1 Model 1: VADER sentiment analyzer\n",
    "This is a model that we didn't even had to train, as it uses the VADER sentiment scores to score each text on how positive/negative it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdf9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to apply VADER sentiment analysis to a text string\n",
    "def get_sentiment(text):\n",
    "    text_str = str(text) # convert to string\n",
    "    sentiment = analyzer.polarity_scores(text_str)\n",
    "    if sentiment['compound'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define a user-defined function (UDF) to apply the get_sentiment function to the review_text column of a dataframe\n",
    "udf_VADER = udf(get_sentiment, tp.StringType())\n",
    "\n",
    "# Define a function to process a Spark RDD using VADER sentiment analysis\n",
    "def process_VADER(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        print(\"rdd was empty...\")\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert RDD to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # Display the dataframe\n",
    "    df.show()\n",
    "\n",
    "    # Apply the udf_VADER function to the review_text column and add the 'pred' column\n",
    "    df_withpreds = df.withColumn(\"pred\", udf_VADER( struct(df.review_text) ))\n",
    "    \n",
    "    # Display the updated dataframe\n",
    "    df_withpreds.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192167b",
   "metadata": {},
   "source": [
    "### 2.2 Model 2: Logistic regression model\n",
    "\n",
    "General structure (data pipeline):\n",
    "* Tokenize review\n",
    "* 'Translate' words into vector representation\n",
    "* Apply a logistic regression model to with vector representations as regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54bbfb5",
   "metadata": {},
   "source": [
    "First we define and create the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358c05ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+\n",
      "|review_id| app_id|         review_text|label|\n",
      "+---------+-------+--------------------+-----+\n",
      "|136510302|2103530|I simply love it,...|    1|\n",
      "|136509602|2349550|  Gifted word: Grace|    1|\n",
      "|136510134|1685730|It's pretty good!...|    1|\n",
      "|136510117|1685730|I recommend it, I...|    1|\n",
      "|136509657|2364130|simple point and ...|    0|\n",
      "+---------+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- review_id: integer (nullable = true)\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "# Define the schema for the data set. \n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'review_id', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'app_id', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'review_text', dataType= tp.StringType(), nullable= True),\n",
    "  tp.StructField(name= 'label', dataType= tp.IntegerType(), nullable= True)\n",
    "])\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "my_data = spark.read.csv(\"C:/Users/wille/spark/MyData/review_data.csv\",\n",
    "                         schema=my_schema,\n",
    "                         header=True)\n",
    "\n",
    "# Probably because of the way the data set was stored as a .csv-file, the extremely long reviews are stored improperly,\n",
    "# leading to missing values. Therefore, we drop these cases from the data set.\n",
    "my_data = my_data.dropna()\n",
    "\n",
    "# view the data\n",
    "my_data.show(5)\n",
    "\n",
    "# print the schema of the file\n",
    "print(\"Schema:\")\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721ebc2",
   "metadata": {},
   "source": [
    "Then we build a logistic regression model to classify sentiment of text reviews. We do this in 4 stages: \n",
    "\n",
    "Stage 1: Tokenize the text of the review. This is done using a regular expression tokenizer that splits the text into individual words or tokens, removing non-word characters, such as punctuation and digits. The input column is 'review_text', and the output column is 'tokens'.\n",
    "\n",
    "Stage 2: Remove stop words from the tokens generated in stage 1. Stop words are commonly occurring words, such as 'the' and 'and', that are typically not useful for analysis. The input column is 'tokens', and the output column is 'filtered_words'.\n",
    "\n",
    "Stage 3: Create a word vector of size 50. This is done using the Word2Vec algorithm, which converts the filtered words from stage 2 into numerical vectors of a specified size. This stage generates a new column called 'vector'.\n",
    "\n",
    "Stage 4: Build a Logistic Regression model to classify sentiment using the vectors generated in stage 3 as input features and the 'label' column as the target variable. This stage defines the model object, with the input features as 'vector' and the target variable as 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17852dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model will consist of several stages, elaborated further below.\n",
    "\n",
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'review_text' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "\n",
    "# define stage 3: create a word vector of the size 50\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize=50)\n",
    "\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5b746",
   "metadata": {},
   "source": [
    "Next we create a pipeline, which is a way to organize multiple stages of processing that transform the data from one form to another, and then fit the pipeline to the training data.\n",
    "\n",
    "The pipeline is created using the Pipeline function from the pyspark.ml module, and is composed of four stages, as defined in the stages list: stage_1, stage_2, stage_3, and model.Model is a LogisticRegression object that trains a binary classification model using the word vectors as features and the label column as the target variable.\n",
    "\n",
    "Once the pipeline is defined, the fit method is used to fit the pipeline to the my_data dataset. This means that the data is transformed by passing it through the pipeline in order, and then used to train the LogisticRegression model. The resulting fitted pipeline object is stored in the pipelineFit variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9aeace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we combine each of these stages together into a data pipeline...\n",
    "pipeline = Pipeline(stages = [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# and fit the model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f228b0c",
   "metadata": {},
   "source": [
    "Next, we define a function process_logistic_regression that takes in a time parameter and an RDD rdd containing review data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee395422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, transform\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def process_logistic_regression(time, rdd):\n",
    "    # Check if RDD is empty\n",
    "    if rdd.isEmpty():\n",
    "        print(\"rdd was empty\")\n",
    "        return\n",
    "    \n",
    "    # Print time and convert to DataFrame\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Make prediction on data frame based on learned model\n",
    "    out = pipelineFit.transform(df).select(\"prediction\")\n",
    "    \n",
    "    # Combine the prediction (which is a data frame) with the original data frame. After searching for a long time on\n",
    "    # how to do this, the only solution we found is this incredibly convoluted way (first append index column to both\n",
    "    # data frames, then merge them based on this index column, then drop the index column).\n",
    "    out = out.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    df = df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    df_withpreds = df.join(out, on=[\"row_index\"]).drop(\"row_index\")\n",
    "    \n",
    "    # Show data frame again, alongside the predictions.\n",
    "    df_withpreds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038a59e",
   "metadata": {},
   "source": [
    "## 3. Make predictions\n",
    "\n",
    "Now we are ready to make real-time predictions \n",
    "The code sets up a streaming context (ssc) using Spark Streaming with a batch interval of 10 seconds. Then it creates a DStream (lines) by connecting to a socket and listening to incoming data on the specified host and port (\"seppe.net\", 7778).\n",
    "\n",
    "Next, the foreachRDD() method is called on the DStream, which applies the specified function (process_logistic_regression()) to each RDD in the stream.\n",
    "\n",
    "Finally, a new StreamingThread object (ssc_t) is created using the ssc object as input, and the thread is started (ssc_t.start()). This allows the stream to start receiving data and processing it using the specified function.\n",
    "\n",
    "The last line ssc_t.stop() is redundant and not necessary for the streaming process. It simply stops the StreamingThread object created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b737716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming context with a batch interval of 10 seconds\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# Set up a socket stream on a specified host and port\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "\n",
    "# Specify which function to apply to each RDD in the stream\n",
    "# Fill in either 'process_VADER' or 'process_logistic_regression'\n",
    "lines.foreachRDD(process_logistic_regression)\n",
    "\n",
    "# Create a thread to run the streaming context\n",
    "ssc_t = StreamingThread(ssc)\n",
    "\n",
    "# Start the streaming context\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69cfe112",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "# Stop the streaming context\n",
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
